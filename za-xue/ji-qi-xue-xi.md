# 机器学习

机器学习基础流程：定义模型（函数），定义损失函数，调整特征，样本等使结果趋于正确。\
lr模型函数fx = wx + b, 区间为实数集，最终需要预测概率，sigmod函数用于将fx最终结果区间归到0到1区间。损失函数用于指导模型如何训练出来w，或者按照什么方法训练出w。其实就是定义了预测结果与真实值的loss是多少。\


交叉熵是损失函数的一种。目前工程用softmax做归一化更多，因为softmax在k=2时候可以退化为sigmoid，多分类扩展性更好。常见word2vec等都用softmax归一化

\
梯度下降函数就是损失函数的偏导。即 loss/w （&是阿尔法，打不出来）\
w的更新就是用上一次训练值减去下一次的偏导。w = w0 - \&loss/w 这个就是典型的优化函数，即求出最后的w。现有优化函数比如adam等是在偏导前面增加了学习率等函数的变化（步长），即这个偏导对w的影响有多大。因为凸函数来说，越靠近正确值，偏导应该越小，固定的偏导可能导致最后反复调整。\
激活函数：如果没有激活函数，对于多层网络来说，每一层函数如果是线性的话，最终无论多少层，本质相乘起来都是线性函数。因此需要激活函数来变化变成非线性函数。比如sigmoid，ReLU等\
损失函数：就是告诉模型真实值与训练值的差距是多少的度量

一个三层网络的深度学习模型，比如第一层函数f1x，第二层函数f2x，第三层函数f3x，偏导数为f1/x1, f2/x2, f3/x3。\
如果出现梯度消失的情况，会使最终训练的w不再更新，如果出现梯度爆炸，偏导会变得不确定，训练出来的w会跳变，不收敛。

协同过滤的向量生成：计算i2i的相似度，比如抽取10万个user，每个user固定位置，买过这个品就是1，没买过这个品就是0。这样每个品就可以产生10w维的向量，也可以计算i2i相似度了\
word2vec：针对user seq进行分词，每个词都会初始化一个向量出来（模型实现的，不一定是全0，初始化的分布满足正态分布），在网络里进行学习，词之间可以进行更新，比如【w1，w2，w3】，w2经常出现在w1，w3中间，这样w1和w3就会更新w2的向量。其他词同理\
双塔向量的生成：与word2vec基本同理，初始化向量，但是用的是用户特征，item特征等训练，结合loss函数等进行更新。

协同过滤缺点：

1. 无行为无法预测：冷启动
2. 行为丰富：哈利波特问题。比如iphone可能所有用户都买，在协同过滤场景，跟每个item都产生了关联，这样推荐时候会给每个用户都推荐\
   推荐系统除了推荐准确性以外，还要考虑新颖性，丰富性问题。准确性可以通过准召率来评估，新颖可以通过多路召回等来弥补。user cf比item cf新颖性更好，但是准确性弱。i2i工程难度小，实时性强，是match常用算法。

模型大体有两类：分类，回归。目前无论是召回还是打分，都是分类模型。只是最后用sigmoid等归一函数所以看起来像回归。\
区分分类和回归主要看数据的原始样本是什么。如果数据原始样本是0和1这种可枚举结果的分类的样本（比如用户点没点击过这个品），那这就是分类问题；如果样本非常丰富（用户点击这个品的ctr是多少）这是回归问题。\
在推荐的时候，思路都是往分类发展，即使碰到回归也尽量往回归的路线拆解问题。\
回归的场景：流量预估，或者预估视频的浏览时长，这种。

{% embed url="https://zhuanlan.zhihu.com/p/61032148" %}

{% embed url="https://zhuanlan.zhihu.com/p/35709485" %}

{% embed url="https://blog.csdn.net/u013250416/article/details/81392161" %}

